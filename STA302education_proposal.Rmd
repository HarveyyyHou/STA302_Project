---
title: "How school type, language, special education services, family income, and parental education level influence OSSLT first attempt results in Ontario schools"
subtitle: "STA302 Final Project Part 1"
author: 
  - Xuanle Zhou
  - Luhan Wang
  - Junyi Hou
date: today
date-format: long
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

# install.packages("psych")
# install.packages("knitr")
# install.packages("kableExtra")
## install.packages("readxl") 
```

```{r}
#| include: false
#| warning: false
#| message: false

library(dplyr)
library(psych)
library(knitr)
library(kableExtra)
library(ggplot2)
library(gridExtra)

```


# Introduction

The Ontario Secondary School English Literacy Test (OSSLT) is mandatory for high school graduation in Ontario, therefore English language learning is a significant focus for both parents and students. This paper aims to investigate how school type, language, special education services, family income, and parental education level influence OSSLT first attempt results in Ontario schools. @zhang2020family found that family income and parental education level significantly contribute to a student’s academic success. Their study was conducted in China, and revealed that higher family income and more advanced parental education are correlated with better student performance. This supports and shapes our hypothesis that students with higher family income and parental education level will perform better on the OSSLT. @bernhofer2022effect showed that students perform better when taught in their first language, which questions if English language school students will perform better on the OSSLT compared to those in non-English language schools. Lastly, @aseery2024enhancing explored how technology and multimedia elements in religious education classes could enhance English language learning. Aseery’s findings suggest that multimedia tools in religious education classes improve student engagement and motivation, which enhances learning outcomes. We would expect schools supplying these technologies in 2025. Therefore, we hypothesize that religious schools will have higher OSSLT pass rates.

While @zhang2020family concluded that higher income and parental education level lead to higher achievement, there are exceptions, as many successful individuals come from lower-income backgrounds. We also expect that students receiving special education services may perform worse on the OSSLT due to specific learning disabilities, despite receiving accommodations. This research question fits well with the concept of multiple linear regression, which examines how multiple predictor variables collaboratively influence a response variable. Therefore, we have selected multiple linear regression as our analysis method. Since the main goal is to observe patterns between variables, this model will focus on interpretability.

This research will benefit those seeking an accurate analysis of the factors that influence English learning outcomes, particularly in the context of the OSSLT. The response variable, OSSLT results, serves as an effective measure of students' English proficiency, as it is both a pass/fail test and provides continuous data.

# Data description

The dataset, available on the Ontario Data Catalogue [@OntarioSchoolData], provides insights into schools in Ontario, supporting policy-making, and educational research. This study repurposes it to investigate and predict the OSSLT first-attempt pass rate. Data were collected from schools, school boards, EQAO, and Statistics Canada through online forms, surveys, phone interviews, and in-person visits, then compiled by Ontario Data Catalogue [@OntarioFindSchool].

```{r}
#| include: false
#| warning: false
#| message: false
data <- read.csv("C:/Users/houju/Desktop/Data_Analysis_SelfLearn/STA302 Part 1 Submission/STA302 Part 1 Submission/STA302_original_dataset.csv")

```

```{r}
#| include: false
#| warning: false
#| message: false
df_clean <- data %>%
  select(`OSSLT_First_Attempt_PassRate`, 
         `School.Type`, 
         `School.Language`, 
         `Percentage.of.Students.Receiving.Special.Education.Services`, 
         `Percentage.of.Students.Whose.Parents.Have.No.Degree..Diploma.or.Certificate`,
         `Percentage.of.School.Aged.Children.Who.Live.in.Low.Income.Households`)

```

```{r}
#| include: false
#| warning: false
#| message: false
# Remove any non-numeric characters (e.g., percentage signs, commas)
df_clean$`OSSLT_First_Attempt_PassRate` <- gsub("[^0-9.]", "", df_clean$`OSSLT_First_Attempt_PassRate`)

# Convert to numeric
df_clean$`OSSLT_First_Attempt_PassRate` <- as.numeric(df_clean$`OSSLT_First_Attempt_PassRate`)

```

```{r}
#| include: false
#| warning: false
#| message: false
df_clean <- df_clean %>%
  mutate(
    `School.Type` = as.factor(`School.Type`),
    `School.Language` = as.factor(`School.Language`)
  )
```

```{r}
#| include: false
#| warning: false
#| message: false
df_clean <- df_clean %>%
  mutate(
    `OSSLT_First_Attempt_PassRate` = as.numeric(`OSSLT_First_Attempt_PassRate`),
    `Percentage.of.Students.Receiving.Special.Education.Services` = as.numeric(`Percentage.of.Students.Receiving.Special.Education.Services`),
    `Percentage.of.School.Aged.Children.Who.Live.in.Low.Income.Households` = as.numeric(`Percentage.of.School.Aged.Children.Who.Live.in.Low.Income.Households`),
    `Percentage.of.Students.Whose.Parents.Have.No.Degree..Diploma.or.Certificate` = as.numeric(`Percentage.of.Students.Whose.Parents.Have.No.Degree..Diploma.or.Certificate`)
  )

```

```{r}
#| include: false
#| warning: false
#| message: false
df_clean <- df_clean %>%
  filter(
    !is.na(`OSSLT_First_Attempt_PassRate`) &
    !is.na(`Percentage.of.Students.Receiving.Special.Education.Services`) &
    !is.na(`Percentage.of.School.Aged.Children.Who.Live.in.Low.Income.Households`) &
    !is.na(`Percentage.of.Students.Whose.Parents.Have.No.Degree..Diploma.or.Certificate`)
  )

# Rename variables in the whole data frame
df_clean <- df_clean %>%
  rename(
    School_Type = School.Type,
    School_Language = School.Language,
    Special_Ed_Pct = Percentage.of.Students.Receiving.Special.Education.Services,
    Low_Income_Pct = Percentage.of.School.Aged.Children.Who.Live.in.Low.Income.Households,
    No_Parent_Degree_Pct = Percentage.of.Students.Whose.Parents.Have.No.Degree..Diploma.or.Certificate
  )

```


```{r}
#| include: false
#| warning: false
#| message: false

write.csv(df_clean, "cleaned_data_school.csv", row.names = FALSE)
```

The `OSSLT_First_Attempt_PassRate`, the response variable, measures the percentage of students passing Ontario Secondary School Literacy Test on their first attempt, ranging from 0 to 100. The mean of 82.45 and median of 85 indicate high pass rates. The dataset originally had 4,926 observations, reduced to 737 after cleaning, ensuring statistical reliability. Despite being bounded, the pass rate is continuous, suitable for linear regression.

```{r}
#| label: tbl-passrate
#| tbl-cap: "OSSLT First Attempt Pass Rate Descriptive Statistics"
#| echo: false

df_clean <- read.csv("C:/Users/houju/Desktop/Data_Analysis_SelfLearn/STA302 Part 1 Submission/STA302 Part 1 Submission/cleaned_data_school.csv")


summary_table <- describe(df_clean %>% select(`OSSLT_First_Attempt_PassRate`)) %>%  
  select(-vars) %>%  
  mutate(across(where(is.numeric), ~ round(., 2)))  

summary_table %>%  
  kable(
    digits = 2, 
    align = "c", 
    format = "latex",  
    booktabs = TRUE,  
    caption.short = ""  
  ) %>%  
  kable_styling(
    latex_options = c("striped", "scale_down"),  
    bootstrap_options = c("striped", "hover", "condensed"),  
    font_size = 12  
  ) %>%  
  column_spec(1, bold = TRUE, width = "5cm") %>%  
  column_spec(2:min(ncol(summary_table), 5), width = "2cm")


```




```{r}
#| label: tbl-histograms
#| tbl-cap: "Histograms for Selected Predictors"
#| echo: false

# Create individual scatter plots
plot1 <- ggplot(df_clean, aes(x = School_Type)) +
  geom_bar(fill = "blue") +
  labs(title = "School Type Count", x = "School Type", y = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(size = 9, face = "bold"))

plot2 <- ggplot(df_clean, aes(x = School_Language)) +
  geom_bar(fill = "red") +
  labs(title = "School Language Count", x = "School Language", y = "Count") +
  theme_minimal()+
  theme(plot.title = element_text(size = 9, face = "bold"))

# Create scatter plots for continuous variables (distribution)
plot3 <- ggplot(df_clean, aes(x = Special_Ed_Pct)) +
  geom_histogram(fill = "lightgreen", bins = 30) +
  labs(title = "Special Education Services", x = "Percentage", y = "Count") +
  theme_minimal()+
  theme(plot.title = element_text(size = 8, face = "bold"))


plot4 <- ggplot(df_clean, aes(x = Low_Income_Pct)) +
  geom_histogram(fill = "purple", bins = 30) +
  labs(title = "Low-Income Households", x = "Percentage", y = "Count") +
  theme_minimal()+
  theme(plot.title = element_text(size = 9, face = "bold"))

plot5 <- ggplot(df_clean, aes(x = No_Parent_Degree_Pct)) +
  geom_histogram(fill = "orange", bins = 30) +
  labs(title = "Parents Without Post-Secondary (%)", x = "Percentage", y = "Count") +
  theme_minimal()+ 
  theme(plot.title = element_text(size = 9, face = "bold"))


# Arrange plots in a 3x2 layout
grid.arrange(plot1, plot2, plot3, plot4, plot5, ncol = 3, nrow = 2)
```

School Type is categorical, with two types: Catholic and Public. Most schools are public. @cheema2024difference noted, private schools generally outperform public schools in literacy. We expect Catholic schools to have higher OSSLT pass rates due to structured curriculum and discipline.

School Language is binary, English or  French. Most schools operate in English, which is expected to correlate with higher OSSLT pass rates.

Students receiving special education services often exhibit lower literacy achievement and slower progress, as noted by @vaughn2014intensive. Our model aims to capture this pattern. The mean of this predictor variable is 24.07%, with a median of 22%, includes outliers where 100% of students receive special education services.

The percentage of school-aged children in low-income households has a mean of 15.27% and skewness of 0.88, indicating some schools have significantly higher concentrations. As @nadeem2021study found, lower-income students often have lower literacy skills, which we expect to correlate with lower OSSLT pass rates.

The percentage of students whose parents lack post-secondary credentials averages 6.76%, with skewness of 1.56 and kurtosis of 3.73, suggesting a slight right skew. As @davis2021role states, parental education influences children’s academic success, making this a relevant predictor.

# Preliminary results


```{r}
#| label: table-regression
#| fig-cap: Regression Results
#| echo: false
#install.packages("car")
library(car)

# Fit the linear model
model <- lm(OSSLT_First_Attempt_PassRate ~ 
             School_Type + 
             School_Language + 
             Special_Ed_Pct + 
             Low_Income_Pct + 
             No_Parent_Degree_Pct, 
             data = df_clean)

# Create a summary table
summary_df <- data.frame(
  Coefficient = coef(model),
  Standard_Error = summary(model)$coefficients[, "Std. Error"],
  t_Statistic = summary(model)$coefficients[, "t value"],
  p_Value = summary(model)$coefficients[, "Pr(>|t|)"]
)

# Display the table
print(summary_df)


```

## Residual Analysis

### Linear Models Assumptions:

1. **Linearity**  
$$
E(Y_i|X = \mathbf{x}_i) = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}
$$

2. **Constant Error Variance (Homoscedasticity)**  
$$
Var(Y_i|X = \mathbf{x}_i) = \sigma^2
$$

3. **Uncorrelated and Normal Errors**  
$$
Cov(e_i, e_j) = 0 \text{ for } i \neq j \text{ and } e_i \sim N(0, \sigma^2)
$$


### Assumption Check
```{r residual-plots}
#| label: plot-residuals
#| fig-cap: Residual Plots
#| echo: false
plot.new
par(mfrow = c(1, 3))  # Arrange plots in one row

# Residuals vs Fitted
plot(model$fitted.values, resid(model), main="Residuals vs Fitted", 
     xlab="Fitted Values", ylab="Residuals", pch=20)
abline(h=0, col="red")

# Q-Q Plot
qqnorm(resid(model), main="Q-Q Plot")
qqline(resid(model), col="red")

# Histogram of Residuals
hist(resid(model), main="Histogram of Residuals", 
     xlab="Residuals", col="gray", border="white")
```


1. **Linearity & Homoscedasticity:** 
  The residuals vs. fitted plot shows no clear pattern, suggesting linearity. Slight heteroscedasticity is observed.

2. **Normality:** 
  The Q-Q plot and the histogram of residuals suggest residuals are approximately normal, though slight deviations exist at the left tail.

3. **Independence:** 
  No evident pattern in the residual plot suggests residuals are independent.


## Model Interpretation & Discussion

### Key Findings and interpretation

- The **intercept (105.18)** represents the estimated pass rate for a **Catholic, English-language school with 0% special education, 0% low-income students, and 0% students whose parents have no degree**. This provides a reference point for understanding the model’s predictions.

- **School Language (French vs. English)** and the three numeric variables (**Special_Ed_Pct, Low_Income_Pct, No_Parent_Degree_Pct**) are strongly associated with the **OSSLT pass rate**.

- **School Type (Public vs. Catholic)** does not show a statistically significant difference in pass rate in this model.

- Higher proportions of **special education students, low-income students, and students whose parents have no degree** are each associated with a **lower pass rate**.

- Conversely, being a **French-language school** is associated with a **higher pass rate** relative to the English.

- The model explains **54% of the variation in pass rates**, which is reasonable for educational data, suggesting these variables collectively have a substantial but not complete ability to predict pass rates.

### Comparison to Literature

Our findings align with prior research while offering insights specific to Ontario:

- **Family Income & Parental Education:** Consistent with @zhang2020family, our results confirm that higher family income and parental education correlate with better OSSLT pass rates.

- **School Language:** Contrary to @bernhofer2022effect, our study shows French-language schools had higher OSSLT pass rates than English-language schools, indicating other factors like curriculum or funding may play a role. Further investigation is needed.

- **Special Education:** Higher proportions of special education students negatively impact OSSLT success, aligning with expectations.

- **School Type:** No significant difference was found between public and Catholic schools, despite @aseery2024enhancing suggesting that religious schools may benefit from enhanced multimedia learning tools.



```{r}
#| label: tbl-predictors
#| tbl-cap: "Descriptive Statistics for Selected Predictors"
#| echo: false



# Create the summary table using the renamed columns
summary_table <- describe(df_clean %>% 
  select(School_Type, 
         School_Language, 
         Special_Ed_Pct, 
         Low_Income_Pct, 
         No_Parent_Degree_Pct)) %>%
  select(-vars) %>%  # Drop the 'vars' column if present
  mutate(across(where(is.numeric), ~ round(., 2)))  # Round numeric values to 2 decimal places


summary_table %>%  
  kable(
    digits = 2, 
    align = "c", 
    format = "latex",  
    booktabs = TRUE,  
    caption.short = ""  
  ) %>%  
  kable_styling(
    latex_options = c("striped", "scale_down"),  
    bootstrap_options = c("striped", "hover", "condensed"),  
    font_size = 12  
  ) %>%  
  column_spec(1, bold = TRUE, width = "5cm") %>%  
  column_spec(2:min(ncol(summary_table), 5), width = "2cm")

```


### Obtain Box Cox Suggestion
```{r}
#| label: Box Cox
#| tbl-cap: "Box Cox"
#| echo: false
df_clean$OSSLT_First_Attempt_PassRate = df_clean$OSSLT_First_Attempt_PassRate +0.001

model <- lm(OSSLT_First_Attempt_PassRate ~ 
             School_Type + 
             School_Language + 
             Special_Ed_Pct + 
             Low_Income_Pct + 
             No_Parent_Degree_Pct, 
             data = df_clean)
library(car)
library(MASS)
boxcox_result <- boxcox(model, plotit = FALSE)  # Plot to find optimal 
lambda <- boxcox_result$x[which.max(boxcox_result$y)]
lambda
```
### Response Variable Transformation Preview with QQ
- We got 2 as Box Cox lambda, Therefore we Try Y^2 Transformation, we also tried many other transformations such as 1/Y, and decided to show Log and Square Root and they are the most improved Models.
```{r residual-plots}
#| label: Response Variable Transformation Preview with QQ
#| fig-cap: Residual Plots
#| echo: false

df_clean$OSSLT_Transform = log(df_clean$OSSLT_First_Attempt_PassRate+0.001)
model_transformed <- lm(OSSLT_Transform ~ School_Type + School_Language + 
                       Special_Ed_Pct + Low_Income_Pct + No_Parent_Degree_Pct, 
                       data = df_clean)
df_clean$OSSLT_Sqrt = sqrt(df_clean$OSSLT_First_Attempt_PassRate)
model_sqrt <- lm(OSSLT_Sqrt ~ School_Type + School_Language + 
                       Special_Ed_Pct + Low_Income_Pct + No_Parent_Degree_Pct, 
                       data = df_clean)
df_clean$OSSLT_Square = (df_clean$OSSLT_First_Attempt_PassRate)^2
model_square <- lm(OSSLT_Square ~ School_Type + School_Language + 
                       Special_Ed_Pct + Low_Income_Pct + No_Parent_Degree_Pct, 
                       data = df_clean)


# Q-Q plot for the original model
p1 = ggplot(data.frame(resid(model)), aes(sample = resid(model))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Q-Q Plot of Residuals")

# Q-Q plot for the transformed model
p2 = ggplot(data.frame(resid(model_transformed)), aes(sample = resid(model_transformed))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Q-Q Plot of Residuals(Log)")

p3 = ggplot(data.frame(resid(model_sqrt)), aes(sample = resid(model_sqrt))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Q-Q Plot of Residuals(Sqrt)")

p4 = ggplot(data.frame(resid(model_square)), aes(sample = resid(model_square))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Q-Q Plot of Residuals(Square)")

grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2)

# summary(model_transformed)
```
### Response Variable Transformation Preview with Residuals
- Square root looks the best for residual plot
```{r residual-plots}
#| label: Response Variable Transformation Preview with Residuals
#| fig-cap: Residual Plots
#| echo: false
library(gridExtra)
# Create individual ggplots
p1 <- ggplot(data.frame(Fitted = model$fitted.values, Residuals = resid(model)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "blue") +
  ggtitle("Original Residuals") +
  theme_minimal()

p2 <- ggplot(data.frame(Fitted = model_transformed$fitted.values, 
                       Residuals = resid(model_transformed)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("Log-Transformed Residuals") +
  theme_minimal()


p3 <- ggplot(data.frame(Fitted = model_sqrt$fitted.values, 
                       Residuals = resid(model_sqrt)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "darkgreen") +
  ggtitle("Sqrt-Transformed Residuals") +
  theme_minimal()

p4 <- ggplot(data.frame(Fitted = model_square$fitted.values, 
                       Residuals = resid(model_square)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "purple") +
  ggtitle("square-Transformed Residuals") +
  theme_minimal()


grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2)

```
### Response Variable Transformation Decision
- We eliminate Square since the model performs worse in every aspect. We need to choose one between Log and Square Root Transformation that both improve from the original model. We ended up with Square root since R squared for square root is significantly higher to Log. The difference in AIC, BIC and AICc is not significant enough to replace R^2.
```{R}
#| label: Y Transformation Decision
#| tbl-cap: "R^2, AIC, BIC, AICc"
#| echo: false
#install.packages("MuMIn")
library(MuMIn)

AIC_original <- AIC(model)
AIC_log <- AIC(model_transformed)
AIC_sqrt <- AIC(model_sqrt)
AIC_square <- AIC(model_square)
BIC_original <- BIC(model)
BIC_log <- BIC(model_transformed)
BIC_sqrt <- BIC(model_sqrt)
BIC_square <- BIC(model_square)
AICc_original <- AICc(model)
AICc_log <- AICc(model_transformed)
AICc_sqrt <- AICc(model_sqrt)
AICc_square <- AICc(model_square)
Ori_R = summary(model)$adj.r.squared
Log_R = summary(model_transformed)$adj.r.squared
Sqrt_R = summary(model_sqrt)$adj.r.squared
Square_R = summary(model_square)$adj.r.squared

# Compare AIC values
ytransform_comparison <- data.frame(
  Model = c("Original", "Log-Transformed", "Sqrt-Transformed","Square"),
  R_Squared = c(Ori_R, Log_R, Sqrt_R, Square_R),
  AIC = c(AIC_original, AIC_log, AIC_sqrt, AIC_square),
  BIC = c(BIC_original, BIC_log, BIC_sqrt, BIC_square),
  AICc = c(AICc_original, AICc_log, AICc_sqrt, AICc_square)
)

# Print the results
print(ytransform_comparison)

```

### Y transformed model summary, VIF and Confidence Interval
- Confidence Interval contains 0 and p value >0.05 for School_Type, Suggest dropping this predictor.
```{r}
#| label: VIF, Confidence Interval and Summary
#| tbl-cap: "VIF, Confidence Interval and Summary"
#| echo: false
summary(model_sqrt)
vif_values <- vif(model_sqrt)
print(vif_values)
confint(model_sqrt)
```

```{r}
#| include: false
#| warning: false
#| message: false
df_clean$Sp_Ed_Transform = (df_clean$Special_Ed_Pct)^2
df_clean$Low_Income_Transform = (df_clean$Low_Income_Pct)
df_clean$No_Edu_Transform = (df_clean$No_Parent_Degree_Pct)
model_Transformwith_x <- lm(OSSLT_Sqrt ~ School_Type + School_Language + Special_Ed_Pct + Sp_Ed_Transform + Low_Income_Transform + No_Edu_Transform, data = df_clean)

model_OneLessPredictor <- lm(OSSLT_Sqrt ~ School_Language + Special_Ed_Pct + Sp_Ed_Transform + Low_Income_Transform + No_Edu_Transform, data = df_clean)

model_drop_No_Add <- lm(OSSLT_Sqrt ~ School_Language + Special_Ed_Pct + Low_Income_Transform + No_Edu_Transform, data = df_clean)

```
### X Transformation QQ Preview
```{R}
#| label: QQ
#| tbl-cap: "QQ of all possible X Transformation"
#| echo: false
p2 = ggplot(data.frame(resid(model_sqrt)), aes(sample = resid(model_sqrt))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Q-Q Plot of Residuals(sqrt)")

p3 = ggplot(data.frame(resid(model_Transformwith_x)), aes(sample = resid(model_Transformwith_x))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("With Polynomial Term On Spec EDU")

p4 = ggplot(data.frame(resid(model_OneLessPredictor)), aes(sample = resid(model_OneLessPredictor))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Removing School Type with extra Term")

p5 = ggplot(data.frame(resid(model_drop_No_Add)), aes(sample = resid(model_drop_No_Add))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Removing School Type")

grid.arrange( p2, p3, p4, p5, ncol = 2)

```

### X Transformation Residual Plots
```{r residual-plots}
#| label: X Transformation Residual Plots
#| fig-cap: Residual Plots
#| echo: false
p2 <- ggplot(data.frame(Fitted = model_sqrt$fitted.values, 
                       Residuals = resid(model_sqrt)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("Sqrt Transformed only Y Residuals") +
  theme_minimal()

p3 <- ggplot(data.frame(Fitted = model_Transformwith_x$fitted.values, 
                       Residuals = resid(model_Transformwith_x)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("With Polynomial Term On Spec EDU") +
  theme_minimal()

p4 <- ggplot(data.frame(Fitted = model_OneLessPredictor$fitted.values, 
                       Residuals = resid(model_OneLessPredictor)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("Removing School Type with extra Term") +
  theme_minimal()

p5 <- ggplot(data.frame(Fitted = model_drop_No_Add$fitted.values, 
                       Residuals = resid(model_drop_No_Add)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("Removing School Type") +
  theme_minimal()

grid.arrange(p2, p3, p4, p5, ncol = 2)
```

### Predictor Transformation Decision
- We decide to choose Model 3: X Transform with one less Predictor, since the R^2, AIC, BIC, AICc improved from the original sqrt-Transformed. Model 2 is slightly better but it contains an extra predictor.
```{R}
#| label: Predictor Transformation Decision
#| tbl-cap: "Predictor Transformation Decision"
#| echo: false
#install.packages("leaps")
library(leaps)
# Extract AIC values (second element of extractAIC() output)
AIC_values <- c(
  extractAIC(model_sqrt)[2],
  extractAIC(model_Transformwith_x)[2],
  extractAIC(model_OneLessPredictor)[2],
  extractAIC(model_drop_No_Add)[2]
)

# Extract other metrics
BIC_values <- c(
  BIC(model_sqrt),
  BIC(model_Transformwith_x),
  BIC(model_OneLessPredictor),
  BIC(model_drop_No_Add)
)

AICc_values <- c(
  AICc(model_sqrt),
  AICc(model_Transformwith_x),
  AICc(model_OneLessPredictor),
  AICc(model_drop_No_Add)
)

R_squared <- c(
  summary(model_sqrt)$adj.r.squared,
  summary(model_Transformwith_x)$adj.r.squared,
  summary(model_OneLessPredictor)$adj.r.squared,
  summary(model_drop_No_Add)$adj.r.squared
)

# Create comparison table
xtransform_comparison <- data.frame(
  Model = c("Sqrt-Transformed", "X Transform", "X Transform with one less Predictor", "One Less Predictor"),
  R_Squared = R_squared,
  AIC = AIC_values,
  BIC = BIC_values,
  AICc = AICc_values
)

# Print results
print(xtransform_comparison)
```

### Anova Table for X Transformation
- Finally, we can conclude the final model with 5 Predictors: School Language, Special Education Percentage, Special Education Percentage Squared, Lower Income background and Parents with no Education. Because RSS improved.
```{R}
#| label: Anova Table for X Transformation
#| tbl-cap: "Anova Table for X Transformation"
#| echo: false
#summary(model_sqrt)
#summary(model_Transformwith_x)
#summary(model_OneLessPredictor)
anova(model_Transformwith_x, model_sqrt)
anova(model_OneLessPredictor, model_sqrt)
anova(model_drop_No_Add, model_sqrt)
```

### Outlier Detection and Removal
- 211,385,102,118,225,484,533. These columns appear under several different tests, so we tried to fit the model after removing these.
```{R}
#| label: Outlier Detection and Removal
#| tbl-cap: "Outlier Detection and Removal"
#| echo: false
# Fit your model
# model_OneLessPredictor
model <- model_OneLessPredictor

# ----------------------------
# 1. Residual Diagnostics
# ----------------------------
# Standardized residuals (for |r_i| > 4)
standardized_resid <- rstandard(model)
std_resid_outliers <- which(abs(standardized_resid) > 4)

# Studentized residuals (for |rstudent| > 3)
rstudent <- rstudent(model)
student_resid_outliers <- which(abs(rstudent) > 3)

# ----------------------------
# 2. Leverage and Influence Metrics
# ----------------------------
# Leverage (hat values)
hat_values <- hatvalues(model)
leverage_threshold <- 2 * (length(coef(model)) + 1) / nobs(model)
high_leverage <- which(hat_values > leverage_threshold)

# Cook's distance
cooks_d <- cooks.distance(model)
cooks_threshold <- 4 / nobs(model)
influential_cooks <- which(cooks_d > cooks_threshold)

# DFFITS
dffits_val <- dffits(model)
dffits_threshold <- 2 * sqrt(length(coef(model)) / nobs(model))
high_dffits <- which(abs(dffits_val) > dffits_threshold)

# DFBETAS
dfbetas_val <- dfbetas(model)
dfbetas_threshold <- 2 / sqrt(nobs(model))
high_dfbetas <- which(apply(abs(dfbetas_val), 1, max) > dfbetas_threshold)

# ----------------------------
# 3. Print Results
# ----------------------------
cat("=== Outliers ===\n")
cat("Standardized residuals (|r_i| > 4):", std_resid_outliers, "\n")
cat("Studentized residuals (|rstudent| > 3):", student_resid_outliers, "\n\n")

cat("=== Influence Metrics ===\n")
cat("High Leverage (Hat >", round(leverage_threshold, 3), "):", high_leverage, "\n")
cat("Influential (Cook's D >", round(cooks_threshold, 3), "):", influential_cooks, "\n")
cat("High DFFITS (>", round(dffits_threshold, 3), "):", high_dffits, "\n")
cat("High DFBETAS (>", round(dfbetas_threshold, 3), "):", high_dfbetas, "\n")

# ----------------------------
# 4. Visualization
# ----------------------------
par(mfrow = c(2, 2))

# Standardized Residuals Plot
plot(standardized_resid, pch = 20, 
     main = "Standardized Residuals (|r_i| > 4)", 
     ylab = "Standardized Residuals")
abline(h = c(-4, 4), col = "red", lty = 2)

# Leverage vs. Standardized Residuals
plot(hat_values, standardized_resid, pch = 20,
     main = "Leverage vs. Standardized Residuals",
     xlab = "Leverage (Hat Values)", ylab = "Standardized Residuals")
abline(h = c(-4, 4), col = "red", lty = 2)
abline(v = leverage_threshold, col = "blue", lty = 2)

# Cook's Distance
plot(cooks_d, pch = 20, main = "Cook's Distance")
abline(h = cooks_threshold, col = "red", lty = 2)

# DFBETAS for the most affected predictor
most_affected_pred <- which.max(apply(abs(dfbetas_val), 2, max))
plot(dfbetas_val[, most_affected_pred], pch = 20,
     main = paste("DFBETAS for", names(coef(model))[most_affected_pred]),
     ylab = "DFBETAS")
abline(h = c(-dfbetas_threshold, dfbetas_threshold), col = "red", lty = 2)
```
- Cleaned Model Summary(Without outliers) and Confidence Interval
```{R}
#| label: Summary
#| tbl-cap: "Cleaned Model Summary(Without outliers) and Confidence Interval"
#| echo: false
#df_clean[c(211, 385), ] 
model_clean <- update(model_OneLessPredictor, 
                     data = df_clean[-c(211, 385, 102, 118, 225, 484, 533), ])
summary(model_clean)
confint(model_clean)
#summary(model_OneLessPredictor)
```

### Assess Model Performance after removing outlier
- Hooray, AIC, BIC, AICc, R^2 all improved significant enough to conclude the removing outlier process.
```{R}
#| label: Assess Model Performance after removing outlier
#| tbl-cap: "Assess Model Performance after removing outlier"
#| echo: false
#Anova, AIC, BIC ...
#install.packages("leaps")
library(leaps)
# Extract AIC values (second element of extractAIC() output)
AIC_values <- c(
  extractAIC(model_OneLessPredictor)[2],
  extractAIC(model_clean)[2]
)

# Extract other metrics
BIC_values <- c(
  BIC(model_OneLessPredictor),
  BIC(model_clean)
)

AICc_values <- c(
  AICc(model_OneLessPredictor),
  AICc(model_clean)
)

R_squared <- c(
  summary(model_OneLessPredictor)$adj.r.squared,
  summary(model_clean)$adj.r.squared
)

# Create comparison table
Final_comparison <- data.frame(
  Model = c("Sqrt-Transformed", "Outliers Removed"),
  R_Squared = R_squared,
  AIC = AIC_values,
  BIC = BIC_values,
  AICc = AICc_values
)

# Final results
print(Final_comparison)
```


### QQ and Residual comparsion
- Both looks Better
```{r residual-plots}
#| label: QQ and Residual comparsion
#| fig-cap: Residual Plots
#| echo: false
p2 <- ggplot(data.frame(Fitted = model_OneLessPredictor$fitted.values, 
                       Residuals = resid(model_OneLessPredictor)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("Before removing outlier") +
  theme_minimal()

p3 <- ggplot(data.frame(Fitted = model_clean$fitted.values, 
                       Residuals = resid(model_clean)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("After Removing outlier") +
  ylim(-6.25, 2.5) +
  theme_minimal()

p4 = ggplot(data.frame(resid(model_OneLessPredictor)), aes(sample = resid(model_OneLessPredictor))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Q-Q Plot of one less predictor")

p5 = ggplot(data.frame(resid(model_clean)), aes(sample = resid(model_clean))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ylim(-6.25, 2.5) +
  ggtitle("Q-Q Plot of After remove outlier")

grid.arrange( p2, p3, p4, p5, ncol = 2)
```
\newpage

\appendix

# Appendix
```{r}
library(gridExtra)

# Add fitted values to df_clean
df_clean$fitted_values <- model$fitted.values

# Function for ggplot with regression line
ggplot_with_abline <- function(df, xcol, xlabel) {
  ggplot(df, aes(x = .data[[xcol]], y = fitted_values)) +
    geom_point(color = "blue") +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = paste(xlabel, "vs y-hat"), x = xlabel, y = "y-hat") +
    theme_minimal()
}

# Create plots
 # Create boxplot for School_Type
p1 <- ggplot(df_clean, aes(x = School_Type, y = fitted_values)) +
  geom_boxplot(aes(fill = School_Type), outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
  labs(title = "School Type vs y-hat", x = "School Type", y = "y-hat") +
  theme_minimal()

# Create boxplot for School_Language
p2 <- ggplot(df_clean, aes(x = School_Language, y = fitted_values)) +
  geom_boxplot(aes(fill = School_Language), outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
  labs(title = "School Language vs y-hat", x = "School Language", y = "y-hat") +
  theme_minimal()

p3 <- ggplot_with_abline(df_clean, "Special_Ed_Pct", "Special Ed Pct")
p4 <- ggplot_with_abline(df_clean, "Low_Income_Pct", "Low Income Pct")


p5 <- ggplot_with_abline(df_clean, "No_Parent_Degree_Pct", "No Parent Degree Pct")

# Arrange plots in a grid
grid.arrange(p1, p2, p3, p4, p5, ncol = 3, nrow = 2)
```
\newpage
# References
