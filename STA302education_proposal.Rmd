---
title: "How school type, language, special education services, family income, and parental education level influence OSSLT first attempt results in Ontario schools"
subtitle: "STA302 Final Project Part 1"
author: 
  - Xuanle Zhou
  - Luhan Wang
  - Junyi Hou
date: today
date-format: long
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

# install.packages("psych")
# install.packages("knitr")
# install.packages("kableExtra")
## install.packages("readxl") 
```

```{r}
#| include: false
#| warning: false
#| message: false

library(dplyr)
library(psych)
library(knitr)
library(kableExtra)
library(ggplot2)
library(gridExtra)
library(broom)
library(MuMIn) 
library(leaps)

```


# Introduction

The Ontario Secondary School English Literacy Test (OSSLT) is mandatory for high school graduation in Ontario, therefore English language learning is a significant focus for both parents and students. This paper aims to investigate how school type, language, special education services, family income, and parental education level influence OSSLT first attempt results in Ontario schools. @zhang2020family found that family income and parental education level significantly contribute to a student’s academic success. Their study was conducted in China, and revealed that higher family income and more advanced parental education are correlated with better student performance. This supports and shapes our hypothesis that students with higher family income and parental education level will perform better on the OSSLT. @bernhofer2022effect showed that students perform better when taught in their first language, which questions if English language school students will perform better on the OSSLT compared to those in non-English language schools. Lastly, @aseery2024enhancing explored how technology and multimedia elements in religious education classes could enhance English language learning. Aseery’s findings suggest that multimedia tools in religious education classes improve student engagement and motivation, which enhances learning outcomes. We would expect schools supplying these technologies in 2025. Therefore, we hypothesize that religious schools will have higher OSSLT pass rates.

While @zhang2020family concluded that higher income and parental education level lead to higher achievement, there are exceptions, as many successful individuals come from lower-income backgrounds. We also expect that students receiving special education services may perform worse on the OSSLT due to specific learning disabilities, despite receiving accommodations. This research question fits well with the concept of multiple linear regression, which examines how multiple predictor variables collaboratively influence a response variable. Therefore, we have selected multiple linear regression as our analysis method. Since the main goal is to observe patterns between variables, this model will focus on interpretability.

This research will benefit those seeking an accurate analysis of the factors that influence English learning outcomes, particularly in the context of the OSSLT. The response variable, OSSLT results, serves as an effective measure of students' English proficiency, as it is both a pass/fail test and provides continuous data.

# Data description

The dataset, available on the Ontario Data Catalogue [@OntarioSchoolData], provides insights into schools in Ontario, supporting policy-making, and educational research. This study repurposes it to investigate and predict the OSSLT first-attempt pass rate. Data were collected from schools, school boards, EQAO, and Statistics Canada through online forms, surveys, phone interviews, and in-person visits, then compiled by Ontario Data Catalogue [@OntarioFindSchool].

```{r}
#| include: false
#| warning: false
#| message: false
data <- read.csv("C:/Users/houju/Desktop/Data_Analysis_SelfLearn/STA302 Part 1 Submission/STA302 Part 1 Submission/STA302_original_dataset.csv")

```

```{r}
#| include: false
#| warning: false
#| message: false
df_clean <- data %>%
  select(`OSSLT_First_Attempt_PassRate`, 
         `School.Type`, 
         `School.Language`, 
         `Percentage.of.Students.Receiving.Special.Education.Services`, 
         `Percentage.of.Students.Whose.Parents.Have.No.Degree..Diploma.or.Certificate`,
         `Percentage.of.School.Aged.Children.Who.Live.in.Low.Income.Households`)

```

```{r}
#| include: false
#| warning: false
#| message: false
# Remove any non-numeric characters (e.g., percentage signs, commas)
df_clean$`OSSLT_First_Attempt_PassRate` <- gsub("[^0-9.]", "", df_clean$`OSSLT_First_Attempt_PassRate`)

# Convert to numeric
df_clean$`OSSLT_First_Attempt_PassRate` <- as.numeric(df_clean$`OSSLT_First_Attempt_PassRate`)

```

```{r}
#| include: false
#| warning: false
#| message: false
df_clean <- df_clean %>%
  mutate(
    `School.Type` = as.factor(`School.Type`),
    `School.Language` = as.factor(`School.Language`)
  )
```

```{r}
#| include: false
#| warning: false
#| message: false
df_clean <- df_clean %>%
  mutate(
    `OSSLT_First_Attempt_PassRate` = as.numeric(`OSSLT_First_Attempt_PassRate`),
    `Percentage.of.Students.Receiving.Special.Education.Services` = as.numeric(`Percentage.of.Students.Receiving.Special.Education.Services`),
    `Percentage.of.School.Aged.Children.Who.Live.in.Low.Income.Households` = as.numeric(`Percentage.of.School.Aged.Children.Who.Live.in.Low.Income.Households`),
    `Percentage.of.Students.Whose.Parents.Have.No.Degree..Diploma.or.Certificate` = as.numeric(`Percentage.of.Students.Whose.Parents.Have.No.Degree..Diploma.or.Certificate`)
  )

```

```{r}
#| include: false
#| warning: false
#| message: false
df_clean <- df_clean %>%
  filter(
    !is.na(`OSSLT_First_Attempt_PassRate`) &
    !is.na(`Percentage.of.Students.Receiving.Special.Education.Services`) &
    !is.na(`Percentage.of.School.Aged.Children.Who.Live.in.Low.Income.Households`) &
    !is.na(`Percentage.of.Students.Whose.Parents.Have.No.Degree..Diploma.or.Certificate`)
  )

# Rename variables in the whole data frame
df_clean <- df_clean %>%
  rename(
    School_Type = School.Type,
    School_Language = School.Language,
    Special_Ed_Pct = Percentage.of.Students.Receiving.Special.Education.Services,
    Low_Income_Pct = Percentage.of.School.Aged.Children.Who.Live.in.Low.Income.Households,
    No_Parent_Degree_Pct = Percentage.of.Students.Whose.Parents.Have.No.Degree..Diploma.or.Certificate
  )

```


```{r}
#| include: false
#| warning: false
#| message: false

write.csv(df_clean, "cleaned_data_school.csv", row.names = FALSE)
```

The `OSSLT_First_Attempt_PassRate`, the response variable, measures the percentage of students passing Ontario Secondary School Literacy Test on their first attempt, ranging from 0 to 100. The mean of 82.45 and median of 85 indicate high pass rates. The dataset originally had 4,926 observations, reduced to 737 after cleaning, ensuring statistical reliability. Despite being bounded, the pass rate is continuous, suitable for linear regression.

```{r}
#| label: tbl-passrate
#| tbl-cap: "OSSLT First Attempt Pass Rate Descriptive Statistics"
#| echo: false

df_clean <- read.csv("C:/Users/houju/Desktop/Data_Analysis_SelfLearn/STA302 Part 1 Submission/STA302 Part 1 Submission/cleaned_data_school.csv")


summary_table <- describe(df_clean %>% select(`OSSLT_First_Attempt_PassRate`)) %>%  
  select(-vars) %>%  
  mutate(across(where(is.numeric), ~ round(., 2)))  

summary_table %>%  
  kable(
    digits = 2, 
    align = "c", 
    format = "latex",  
    booktabs = TRUE,  
    caption.short = ""  
  ) %>%  
  kable_styling(
    latex_options = c("striped", "scale_down"),  
    bootstrap_options = c("striped", "hover", "condensed"),  
    font_size = 12  
  ) %>%  
  column_spec(1, bold = TRUE, width = "5cm") %>%  
  column_spec(2:min(ncol(summary_table), 5), width = "2cm")

summary_table
```




```{r}
#| label: tbl-histograms
#| tbl-cap: "Histograms for Selected Predictors"
#| echo: false

# Create individual scatter plots
plot1 <- ggplot(df_clean, aes(x = School_Type)) +
  geom_bar(fill = "blue") +
  labs(title = "School Type Count", x = "School Type", y = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(size = 9, face = "bold"))

plot2 <- ggplot(df_clean, aes(x = School_Language)) +
  geom_bar(fill = "red") +
  labs(title = "School Language Count", x = "School Language", y = "Count") +
  theme_minimal()+
  theme(plot.title = element_text(size = 9, face = "bold"))

# Create scatter plots for continuous variables (distribution)
plot3 <- ggplot(df_clean, aes(x = Special_Ed_Pct)) +
  geom_histogram(fill = "lightgreen", bins = 30) +
  labs(title = "Special Education Services", x = "Percentage", y = "Count") +
  theme_minimal()+
  theme(plot.title = element_text(size = 8, face = "bold"))


plot4 <- ggplot(df_clean, aes(x = Low_Income_Pct)) +
  geom_histogram(fill = "purple", bins = 30) +
  labs(title = "Low-Income Households", x = "Percentage", y = "Count") +
  theme_minimal()+
  theme(plot.title = element_text(size = 9, face = "bold"))

plot5 <- ggplot(df_clean, aes(x = No_Parent_Degree_Pct)) +
  geom_histogram(fill = "orange", bins = 30) +
  labs(title = "Parents Without Post-Secondary (%)", x = "Percentage", y = "Count") +
  theme_minimal()+ 
  theme(plot.title = element_text(size = 9, face = "bold"))


# Arrange plots in a 3x2 layout
grid.arrange(plot1, plot2, plot3, plot4, plot5, ncol = 3, nrow = 2)
```

School Type is categorical, with two types: Catholic and Public. Most schools are public. @cheema2024difference noted, private schools generally outperform public schools in literacy. We expect Catholic schools to have higher OSSLT pass rates due to structured curriculum and discipline.

School Language is binary, English or  French. Most schools operate in English, which is expected to correlate with higher OSSLT pass rates.

Students receiving special education services often exhibit lower literacy achievement and slower progress, as noted by @vaughn2014intensive. Our model aims to capture this pattern. The mean of this predictor variable is 24.07%, with a median of 22%, includes outliers where 100% of students receive special education services.

The percentage of school-aged children in low-income households has a mean of 15.27% and skewness of 0.88, indicating some schools have significantly higher concentrations. As @nadeem2021study found, lower-income students often have lower literacy skills, which we expect to correlate with lower OSSLT pass rates.

The percentage of students whose parents lack post-secondary credentials averages 6.76%, with skewness of 1.56 and kurtosis of 3.73, suggesting a slight right skew. As @davis2021role states, parental education influences children’s academic success, making this a relevant predictor.

# Preliminary results


```{r}
#| label: table-regression
#| fig-cap: Regression Results
#| echo: false
#install.packages("car")
library(car)

# Fit the linear model
model <- lm(OSSLT_First_Attempt_PassRate ~ 
             School_Type + 
             School_Language + 
             Special_Ed_Pct + 
             Low_Income_Pct + 
             No_Parent_Degree_Pct, 
             data = df_clean)

# Create a summary table
summary_df <- data.frame(
  Coefficient = coef(model),
  Standard_Error = summary(model)$coefficients[, "Std. Error"],
  t_Statistic = summary(model)$coefficients[, "t value"],
  p_Value = summary(model)$coefficients[, "Pr(>|t|)"]
)

# Display the table
print(summary_df)


```

## Residual Analysis

### Linear Models Assumptions:

1. **Linearity**  
$$
E(Y_i|X = \mathbf{x}_i) = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}
$$

2. **Constant Error Variance (Homoscedasticity)**  
$$
Var(Y_i|X = \mathbf{x}_i) = \sigma^2
$$

3. **Uncorrelated and Normal Errors**  
$$
Cov(e_i, e_j) = 0 \text{ for } i \neq j \text{ and } e_i \sim N(0, \sigma^2)
$$


### Assumption Check
```{r residual-plots}
#| label: plot-residuals
#| fig-cap: Residual Plots
#| echo: false
plot.new
par(mfrow = c(1, 3))  # Arrange plots in one row

# Residuals vs Fitted
plot(model$fitted.values, resid(model), main="Residuals vs Fitted", 
     xlab="Fitted Values", ylab="Residuals", pch=20)
abline(h=0, col="red")

# Q-Q Plot
qqnorm(resid(model), main="Q-Q Plot")
qqline(resid(model), col="red")

# Histogram of Residuals
hist(resid(model), main="Histogram of Residuals", 
     xlab="Residuals", col="gray", border="white")
```


1. **Linearity & Homoscedasticity:** 
  The residuals vs. fitted plot shows no clear pattern, suggesting linearity. Slight heteroscedasticity is observed.

2. **Normality:** 
  The Q-Q plot and the histogram of residuals suggest residuals are approximately normal, though slight deviations exist at the left tail.

3. **Independence:** 
  No evident pattern in the residual plot suggests residuals are independent.


## Model Interpretation & Discussion

### Key Findings and interpretation

- The **intercept (105.18)** represents the estimated pass rate for a **Catholic, English-language school with 0% special education, 0% low-income students, and 0% students whose parents have no degree**. This provides a reference point for understanding the model’s predictions.

- **School Language (French vs. English)** and the three numeric variables (**Special_Ed_Pct, Low_Income_Pct, No_Parent_Degree_Pct**) are strongly associated with the **OSSLT pass rate**.

- **School Type (Public vs. Catholic)** does not show a statistically significant difference in pass rate in this model.

- Higher proportions of **special education students, low-income students, and students whose parents have no degree** are each associated with a **lower pass rate**.

- Conversely, being a **French-language school** is associated with a **higher pass rate** relative to the English.

- The model explains **54% of the variation in pass rates**, which is reasonable for educational data, suggesting these variables collectively have a substantial but not complete ability to predict pass rates.

### Comparison to Literature

Our findings align with prior research while offering insights specific to Ontario:

- **Family Income & Parental Education:** Consistent with @zhang2020family, our results confirm that higher family income and parental education correlate with better OSSLT pass rates.

- **School Language:** Contrary to @bernhofer2022effect, our study shows French-language schools had higher OSSLT pass rates than English-language schools, indicating other factors like curriculum or funding may play a role. Further investigation is needed.

- **Special Education:** Higher proportions of special education students negatively impact OSSLT success, aligning with expectations.

- **School Type:** No significant difference was found between public and Catholic schools, despite @aseery2024enhancing suggesting that religious schools may benefit from enhanced multimedia learning tools.



```{r}
#| label: tbl-predictors
#| tbl-cap: "Descriptive Statistics for Selected Predictors"
#| echo: false



# Create the summary table using the renamed columns
summary_table <- describe(df_clean %>% 
  select(School_Type, 
         School_Language, 
         Special_Ed_Pct, 
         Low_Income_Pct, 
         No_Parent_Degree_Pct)) %>%
  select(-vars) %>%  # Drop the 'vars' column if present
  mutate(across(where(is.numeric), ~ round(., 2)))  # Round numeric values to 2 decimal places


summary_table %>%  
  kable(
    digits = 2, 
    align = "c", 
    format = "latex",  
    booktabs = TRUE,  
    caption.short = ""  
  ) %>%  
  kable_styling(
    latex_options = c("striped", "scale_down"),  
    bootstrap_options = c("striped", "hover", "condensed"),  
    font_size = 12  
  ) %>%  
  column_spec(1, bold = TRUE, width = "5cm") %>%  
  column_spec(2:min(ncol(summary_table), 5), width = "2cm")

summary_table
```

# Model Selection
```{r}
#| include: false
#| warning: false
#| message: false
df_clean$OSSLT_First_Attempt_PassRate = df_clean$OSSLT_First_Attempt_PassRate +0.001

model <- lm(OSSLT_First_Attempt_PassRate ~ 
             School_Type + 
             School_Language + 
             Special_Ed_Pct + 
             Low_Income_Pct + 
             No_Parent_Degree_Pct, 
             data = df_clean)
library(car)
library(MASS)
boxcox_result <- boxcox(model, plotit = FALSE)  # Plot to find optimal 
lambda <- boxcox_result$x[which.max(boxcox_result$y)]
lambda
```
### Response Variable Transformation and Assumption Comparison
- After fitting our original model, we consider some transformations. The first method is Box Cox, which uses maximum likelihood to choose transformation so the residuals are approximately normally distributed. We found 2 as Box Cox lambda, suggesting a squared transformation. Therefore we Try Y^2 Transformation, we also tried many other transformations such as 1/Y, and decided to display log and square root as they improved from original model.
- Normality: The Q-Q plot of residuals suggest residuals are approximately normal for square root and log, though slight deviations exist at the left tail. Note that the scale for the Square root graph is a lot bigger, minimal deviation indicates violation.
```{r residual-plots}
#| label: Response Variable Transformation Preview with QQ
#| fig-cap: Residual Plots
#| echo: false

df_clean$OSSLT_Transform = log(df_clean$OSSLT_First_Attempt_PassRate+0.001)
model_transformed <- lm(OSSLT_Transform ~ School_Type + School_Language + 
                       Special_Ed_Pct + Low_Income_Pct + No_Parent_Degree_Pct, 
                       data = df_clean)
df_clean$OSSLT_Sqrt = sqrt(df_clean$OSSLT_First_Attempt_PassRate)
model_sqrt <- lm(OSSLT_Sqrt ~ School_Type + School_Language + 
                       Special_Ed_Pct + Low_Income_Pct + No_Parent_Degree_Pct, 
                       data = df_clean)
df_clean$OSSLT_Square = (df_clean$OSSLT_First_Attempt_PassRate)^2
model_square <- lm(OSSLT_Square ~ School_Type + School_Language + 
                       Special_Ed_Pct + Low_Income_Pct + No_Parent_Degree_Pct, 
                       data = df_clean)


# Q-Q plot for the original model
p1 = ggplot(data.frame(resid(model)), aes(sample = resid(model))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Q-Q Plot of Residuals")

# Q-Q plot for the transformed model
p2 = ggplot(data.frame(resid(model_transformed)), aes(sample = resid(model_transformed))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Q-Q Plot of Residuals(Log)")

p3 = ggplot(data.frame(resid(model_sqrt)), aes(sample = resid(model_sqrt))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Q-Q Plot of Residuals(Sqrt)")

p4 = ggplot(data.frame(resid(model_square)), aes(sample = resid(model_square))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Q-Q Plot of Residuals(Square)")

grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2)

# summary(model_transformed)
```
### Response Variable Transformation Preview with Residuals
- Linearity & Homoscedasticity: The Square Root residuals vs. fitted plot is ideal, as the Y scale range is a lot smaller than other transformations and there is no clear patterns. This suggest linearity. Slight heteroscedasticity is observed around x=-10.
```{r residual-plots}
#| label: Response Variable Transformation Preview with Residuals
#| fig-cap: Residual Plots
#| echo: false
library(gridExtra)
# Create individual ggplots
p1 <- ggplot(data.frame(Fitted = model$fitted.values, Residuals = resid(model)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "blue") +
  ggtitle("Original Residuals") +
  theme_minimal()

p2 <- ggplot(data.frame(Fitted = model_transformed$fitted.values, 
                       Residuals = resid(model_transformed)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("Log-Transformed Residuals") +
  theme_minimal()


p3 <- ggplot(data.frame(Fitted = model_sqrt$fitted.values, 
                       Residuals = resid(model_sqrt)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "darkgreen") +
  ggtitle("Sqrt-Transformed Residuals") +
  theme_minimal()

p4 <- ggplot(data.frame(Fitted = model_square$fitted.values, 
                       Residuals = resid(model_square)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "purple") +
  ggtitle("square-Transformed Residuals") +
  theme_minimal()


grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2)

```
### Response Variable Transformation Metrics and Decision
- We eliminate Square since the model performs worse in every aspect. Square Root and Log Transformation both rapidly improve from the original model based on AIC, BIC and AICc. However, R^2 dropped by approximately 34% compared to original for Log, indicating log model's ability to explain variance significantly decreased. In contrast, Square Root Model R^2 only decreased by less than 2% for better Normality, linearity and Homoscedasticity. The difference in AIC, BIC and AICc between Log and Square Root is not significant enough to replace R^2. Therefore, we apply square root transformation on our response variable.
```{R}
#| label: Y Transformation Decision
#| tbl-cap: "R^2, AIC, BIC, AICc"
#| echo: false
#install.packages("MuMIn")
library(MuMIn)

AIC_original <- AIC(model)
AIC_log <- AIC(model_transformed)
AIC_sqrt <- AIC(model_sqrt)
AIC_square <- AIC(model_square)
BIC_original <- BIC(model)
BIC_log <- BIC(model_transformed)
BIC_sqrt <- BIC(model_sqrt)
BIC_square <- BIC(model_square)
AICc_original <- AICc(model)
AICc_log <- AICc(model_transformed)
AICc_sqrt <- AICc(model_sqrt)
AICc_square <- AICc(model_square)
Ori_R = summary(model)$adj.r.squared
Log_R = summary(model_transformed)$adj.r.squared
Sqrt_R = summary(model_sqrt)$adj.r.squared
Square_R = summary(model_square)$adj.r.squared

# Compare AIC values
ytransform_comparison <- data.frame(
  Model = c("Original", "Log-Transformed", "Sqrt-Transformed","Square"),
  R_Squared = c(Ori_R, Log_R, Sqrt_R, Square_R),
  AIC = c(AIC_original, AIC_log, AIC_sqrt, AIC_square),
  BIC = c(BIC_original, BIC_log, BIC_sqrt, BIC_square),
  AICc = c(AICc_original, AICc_log, AICc_sqrt, AICc_square)
)

# Print the results
print(ytransform_comparison)

```

### Y transformed model summary, VIF and Confidence Interval
- The confidence interval contains 0 and p value >0.05 for School_Type, suggest dropping this predictor.
- Variance Inflation Factor for all are smaller than 5, indicating no predictor have issues with multicollinearity.
```{r}
#| label: VIF, Confidence Interval and Summary
#| tbl-cap: "VIF, Confidence Interval and Summary"
#| echo: false
# Tidy model output
coef_table <- tidy(model_sqrt)

# Confidence intervals
confint_df <- confint(model_sqrt) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("term") %>%
  rename(`Lower 95% CI` = `2.5 %`, `Upper 95% CI` = `97.5 %`)

# VIFs from model
raw_vifs <- vif(model_sqrt)

# Expand VIFs to match dummy-coded names
expanded_vif <- coef_table %>%
  dplyr::select(term) %>%
  dplyr::mutate(
    base_var = dplyr::case_when(
      grepl("^School_Type", term) ~ "School_Type",
      grepl("^School_Language", term) ~ "School_Language",
      term == "Special_Ed_Pct" ~ "Special_Ed_Pct",
      term == "Low_Income_Pct" ~ "Low_Income_Pct",
      term == "No_Parent_Degree_Pct" ~ "No_Parent_Degree_Pct",
      TRUE ~ NA_character_
    ),
    VIF = raw_vifs[base_var]
  ) %>%
  dplyr::select(term, VIF)

# Merge all
full_table <- coef_table %>%
  left_join(confint_df, by = "term") %>%
  left_join(expanded_vif, by = "term")

# Format values
full_table <- full_table %>%
  mutate(
    `t value` = formatC(statistic, format = "f", digits = 2),
    `Pr(>|t|)` = format.pval(p.value, digits = 3, eps = .Machine$double.eps),
    Estimate = formatC(estimate, format = "f", digits = 4),
    `Std. Error` = formatC(std.error, format = "f", digits = 4),
    `Lower 95% CI` = formatC(`Lower 95% CI`, format = "f", digits = 4),
    `Upper 95% CI` = formatC(`Upper 95% CI`, format = "f", digits = 4),
    VIF = ifelse(is.na(VIF), "", formatC(VIF, format = "f", digits = 2))
  )

# Rename terms for final display
full_table <- full_table %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "(Intercept)",
    term == "School_TypePublic" ~ "School Type - Public",
    term == "School_LanguageFrench" ~ "School Language - French",
    term == "Special_Ed_Pct" ~ "% in Special Education",
    term == "Low_Income_Pct" ~ "% in Low Income",
    term == "No_Parent_Degree_Pct" ~ "% in No Educated Parent",
    TRUE ~ term
  ))

# Display final table
full_table[, c("term", "Estimate", "Std. Error", "t value", "Pr(>|t|)",
               "Lower 95% CI", "Upper 95% CI", "VIF")] %>%
  kable(
    col.names = c("Predictor", "Estimate", "Std. Error", "t value", "p-value",
                  "Lower 95% CI", "Upper 95% CI", "VIF"),
    caption = "Regression Coefficients with 95% Confidence Intervals and VIF",
    escape = FALSE,
    booktabs = TRUE,
    align = "lccccccc"
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "threeparttable"),
    font_size = 9
  ) %>%
  row_spec(0, bold = TRUE)

```

```{r}
#| include: false
#| warning: false
#| message: false
df_clean$Sp_Ed_Transform = (df_clean$Special_Ed_Pct)^2
df_clean$Low_Income_Transform = (df_clean$Low_Income_Pct)
df_clean$No_Edu_Transform = (df_clean$No_Parent_Degree_Pct)
model_Transformwith_x <- lm(OSSLT_Sqrt ~ School_Type + School_Language + Special_Ed_Pct + Sp_Ed_Transform + Low_Income_Transform + No_Edu_Transform, data = df_clean)

model_OneLessPredictor <- lm(OSSLT_Sqrt ~ School_Language + Special_Ed_Pct + Low_Income_Transform + No_Edu_Transform + Sp_Ed_Transform, data = df_clean)

model_drop_No_Add <- lm(OSSLT_Sqrt ~ School_Language + Special_Ed_Pct + Low_Income_Transform + No_Edu_Transform, data = df_clean)

```
### X Transformation Assumption Preview
- Other than dropping School Type, we also consider X transformation. After testing different combinations of X transformations, we found that adding a squared special education term slightly increase model performance. Therefore we compare four models: Original with Sqrt(Y), remove school type, add squared special education, and both remove and add. All X transformations models shows similar plots to the original Square Root(y) model plot. 
```{R}
#| label: QQ
#| tbl-cap: "QQ of all possible X Transformation"
#| echo: false
p2 = ggplot(data.frame(resid(model_sqrt)), aes(sample = resid(model_sqrt))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Q-Q Plot of Residuals(sqrt)") +
  theme(plot.title = element_text(size = 10))

p3 = ggplot(data.frame(resid(model_Transformwith_x)), aes(sample = resid(model_Transformwith_x))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Add Squared Special EDU") +
  theme(plot.title = element_text(size = 10))

p4 = ggplot(data.frame(resid(model_OneLessPredictor)), aes(sample = resid(model_OneLessPredictor))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Drop School Type, Add Squared Spec EDU") +
  theme(plot.title = element_text(size = 10))

p5 = ggplot(data.frame(resid(model_drop_No_Add)), aes(sample = resid(model_drop_No_Add))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Drop School Type") +
  theme(plot.title = element_text(size = 10))

grid.arrange( p2, p3, p4, p5, ncol = 2)

```

```{r residual-plots}
#| label: X Transformation Residual Plots
#| fig-cap: Residual Plots
#| echo: false
p2 <- ggplot(data.frame(Fitted = model_sqrt$fitted.values, 
                       Residuals = resid(model_sqrt)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("Sqrt Transformed only Y Residuals") +
  theme_minimal()

p3 <- ggplot(data.frame(Fitted = model_Transformwith_x$fitted.values, 
                       Residuals = resid(model_Transformwith_x)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("With Polynomial Term On Spec EDU") +
  theme_minimal()

p4 <- ggplot(data.frame(Fitted = model_OneLessPredictor$fitted.values, 
                       Residuals = resid(model_OneLessPredictor)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("Removing School Type with extra Term") +
  theme_minimal()

p5 <- ggplot(data.frame(Fitted = model_drop_No_Add$fitted.values, 
                       Residuals = resid(model_drop_No_Add)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("Removing School Type") +
  theme_minimal()

grid.arrange(p2, p3, p4, p5, ncol = 2)
```

### Predictor Transformation Metrics and Decision
-  R^2, AIC, BIC, AICc and RSS from anova all slightly improved from models that has the Squared Special Education predictor. However, the improvement is not significant enough (large enough) to add a new predictor and complicate the model. 
- Between Original and removing School Type, we decide to drop school type, since all metrics does not differ by much, indicating that this predictor has minimal impact on the model, or no relationships.
- Finally, we can conclude the final model with 4 Predictors: School Language, Special Education Percentage.
```{R}
#| label: Predictor Transformation Decision
#| tbl-cap: "Predictor Transformation Decision"
#| echo: false
#install.packages("leaps")
library(leaps)
# Extract AIC values (second element of extractAIC() output)
AIC_values <- c(
  extractAIC(model_sqrt)[2],
  extractAIC(model_Transformwith_x)[2],
  extractAIC(model_OneLessPredictor)[2],
  extractAIC(model_drop_No_Add)[2]
)

# Extract other metrics
BIC_values <- c(
  BIC(model_sqrt),
  BIC(model_Transformwith_x),
  BIC(model_OneLessPredictor),
  BIC(model_drop_No_Add)
)

AICc_values <- c(
  AICc(model_sqrt),
  AICc(model_Transformwith_x),
  AICc(model_OneLessPredictor),
  AICc(model_drop_No_Add)
)

R_squared <- c(
  summary(model_sqrt)$adj.r.squared,
  summary(model_Transformwith_x)$adj.r.squared,
  summary(model_OneLessPredictor)$adj.r.squared,
  summary(model_drop_No_Add)$adj.r.squared
)

# Create comparison table
xtransform_comparison <- data.frame(
  Model = c("Sqrt-Transformed", "X Transform", "X Transform with one less Predictor", "One Less Predictor"),
  R_Squared = R_squared,
  AIC = AIC_values,
  BIC = BIC_values,
  AICc = AICc_values
)

# Print results
print(xtransform_comparison)
```

```{R}
#| label: Anova Table for X Transformation
#| tbl-cap: "Anova Table for X Transformation"
#| echo: false
PureX_Transform = anova(model_Transformwith_x, model_sqrt)
X_Transform_with_less_Predictor = anova(model_OneLessPredictor, model_sqrt)
Less_Predictor = anova(model_drop_No_Add, model_sqrt)

# Extract RSS and p-values
results <- data.frame(
  Comparison = c(
    "Special Ed Squared vs. Original",
    "Reference: Original Model",
    "No School_Type & X-Trans vs. Full",
    "No School_Type vs. Full"
  ),
  RSS_Model1 = c(
    PureX_Transform$RSS[1],
    NA,
    X_Transform_with_less_Predictor$RSS[1],
    Less_Predictor$RSS[1]
  ),
  RSS_Model2 = c(
    PureX_Transform$RSS[2],
    PureX_Transform$RSS[2],  # Original model RSS (reference)
    X_Transform_with_less_Predictor$RSS[2],
    Less_Predictor$RSS[2]
  ),
  p_value = c(
    PureX_Transform$`Pr(>F)`[2],
    NA,
    NA,  # Df=0, no p-value
    Less_Predictor$`Pr(>F)`[2]
  )
)

# Format p-values for significance stars
results$p_value <- ifelse(
  is.na(results$p_value), 
  "", 
  ifelse(
    results$p_value < 0.001, 
    sprintf("%.2e ***", results$p_value),
    ifelse(
      results$p_value < 0.01, 
      sprintf("%.4f **", results$p_value),
      ifelse(
        results$p_value < 0.05, 
        sprintf("%.4f *", results$p_value),
        sprintf("%.4f", results$p_value)
      )
    )
  )
)

# Print the clean table
print(results)

# Print the combined table
# print(combined_anova)
```

### Outlier Detection and Removal
- To detect outliers, influential and leverage points. We tested observation for standardized and studentized residual for outliers; hat for leverage points; DFFITS, DFBETAS and Cook's Distance for influencial points.
- These columns (211,385,102,118,225,484,533) appear under several different tests, so we try to fit the model after removing these.
```{R}
#| include: false
#| warning: false
#| message: false
# Fit your model
model <- model_drop_No_Add

# ----------------------------
# 1. Residual Diagnostics
# ----------------------------
# Standardized residuals (for |r_i| > 4)
standardized_resid <- rstandard(model)
std_resid_outliers <- which(abs(standardized_resid) > 4)

# Studentized residuals (for |rstudent| > 3)
rstudent <- rstudent(model)
student_resid_outliers <- which(abs(rstudent) > 3)

# ----------------------------
# 2. Leverage and Influence Metrics
# ----------------------------
# Leverage (hat values)
hat_values <- hatvalues(model)
leverage_threshold <- 2 * (length(coef(model)) + 1) / nobs(model)
high_leverage <- which(hat_values > leverage_threshold)

# Cook's distance
cooks_d <- cooks.distance(model)
cooks_threshold <- 4 / nobs(model)
influential_cooks <- which(cooks_d > cooks_threshold)

# DFFITS
dffits_val <- dffits(model)
dffits_threshold <- 2 * sqrt(length(coef(model)) / nobs(model))
high_dffits <- which(abs(dffits_val) > dffits_threshold)

# DFBETAS
dfbetas_val <- dfbetas(model)
dfbetas_threshold <- 2 / sqrt(nobs(model))
high_dfbetas <- which(apply(abs(dfbetas_val), 1, max) > dfbetas_threshold)

# ----------------------------
# 3. Print Results
# ----------------------------
cat("=== Outliers ===\n")
cat("Standardized residuals (|r_i| > 4):", std_resid_outliers, "\n")
cat("Studentized residuals (|rstudent| > 3):", student_resid_outliers, "\n\n")

cat("=== Influence Metrics ===\n")
cat("High Leverage (Hat >", round(leverage_threshold, 3), "):", high_leverage, "\n")
cat("Influential (Cook's D >", round(cooks_threshold, 3), "):", influential_cooks, "\n")
cat("High DFFITS (>", round(dffits_threshold, 3), "):", high_dffits, "\n")
cat("High DFBETAS (>", round(dfbetas_threshold, 3), "):", high_dfbetas, "\n")

# ----------------------------
# 4. Visualization
# ----------------------------
par(mfrow = c(2, 2))

# Standardized Residuals Plot
plot(standardized_resid, pch = 20, 
     main = "Standardized Residuals (|r_i| > 4)", 
     ylab = "Standardized Residuals")
abline(h = c(-4, 4), col = "red", lty = 2)

# Leverage vs. Standardized Residuals
plot(hat_values, standardized_resid, pch = 20,
     main = "Leverage vs. Standardized Residuals",
     xlab = "Leverage (Hat Values)", ylab = "Standardized Residuals")
abline(h = c(-4, 4), col = "red", lty = 2)
abline(v = leverage_threshold, col = "blue", lty = 2)

# Cook's Distance
plot(cooks_d, pch = 20, main = "Cook's Distance")
abline(h = cooks_threshold, col = "red", lty = 2)

# DFBETAS for the most affected predictor
most_affected_pred <- which.max(apply(abs(dfbetas_val), 2, max))
plot(dfbetas_val[, most_affected_pred], pch = 20,
     main = paste("DFBETAS for", names(coef(model))[most_affected_pred]),
     ylab = "DFBETAS")
abline(h = c(-dfbetas_threshold, dfbetas_threshold), col = "red", lty = 2)
```
```{r}
#| label: Outlier Detection and Removal
#| tbl-cap: "Outlier Detection and Removal"
#| echo: false
# Dropped outliers
dropped_ids <- c(118, 211, 225, 383, 385, 486)

# Categories they appear in
outlier_info <- data.frame(
  ID = dropped_ids,
  `Standardized Residual` = dropped_ids %in% c(211),
  `Studentized Residual` = dropped_ids %in% c(118, 211, 225, 383, 385, 486),
  `High Leverage (Hat > 0.016)` = dropped_ids %in% c(118, 211, 225, 383, 385),
  `Influential (Cook's D > 0.005)` = dropped_ids %in% c(118, 211, 225, 383, 385, 486),
  `High DFFITS` = dropped_ids %in% c(118, 211, 225, 383, 385, 486),
  `High DFBETAS` = dropped_ids %in% c(118, 211, 225, 383, 385, 486)
)

# Convert logical TRUE/FALSE to ✔ or ""
outlier_table <- outlier_info %>%
  mutate(across(-ID, ~ ifelse(.x, "✔", "")))

# Show table using kable

outlier_table %>%
  kable(
    caption = "Significant Outliers Found in multiple Categories",
    col.names = c("Observation ID", "Standardized Residual", "Studentized Residual", 
                  "High Leverage", "Influential (Cook's D)", "High DFFITS", "High DFBETAS"),
    align = "c",
    escape = FALSE,
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "threeparttable"),
    full_width = FALSE,
    font_size = 10
  ) %>%
  row_spec(0, bold = TRUE)

```

### Assess Model Performance after removing outlier
- AIC, BIC, AICc, R^2 all improved after removing outliers, so we conclude the removing outlier process.
```{R}
#| label: Assess Model Performance after removing outlier
#| tbl-cap: "Assess Model Performance after removing outlier"
#| echo: false

model_clean <- update(model_drop_No_Add,
                      data = df_clean[-c(118, 211, 225, 383, 385, 486), ])

AIC_values <- c(
  extractAIC(model_drop_No_Add)[2],
  extractAIC(model_clean)[2]
)

# Extract other metrics
BIC_values <- c(
  BIC(model_drop_No_Add),
  BIC(model_clean)
)

AICc_values <- c(
  AICc(model_drop_No_Add),
  AICc(model_clean)
)

R_squared <- c(
  summary(model_drop_No_Add)$adj.r.squared,
  summary(model_clean)$adj.r.squared
)

# Create comparison table
Final_comparison <- data.frame(
  Model = c("Sqrt-Transformed", "Outliers Removed"),
  R_Squared = R_squared,
  AIC = AIC_values,
  BIC = BIC_values,
  AICc = AICc_values
)

# Final results
print(Final_comparison)
```


### QQ and Residual final comparsion
- Both graphs improved with no outliers near the bound of scales.
```{r residual-plots}
#| label: QQ and Residual comparsion
#| fig-cap: Residual Plots
#| echo: false
p2 <- ggplot(data.frame(Fitted = model_drop_No_Add$fitted.values, 
                       Residuals = resid(model_drop_No_Add)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("Before removing outlier") +
  theme_minimal()

p3 <- ggplot(data.frame(Fitted = model_clean$fitted.values, 
                       Residuals = resid(model_clean)), 
             aes(Fitted, Residuals)) +
  geom_point(color = "red") +
  ggtitle("After Removing outlier") +
  ylim(-6.25, 2.5) +
  theme_minimal()

p4 = ggplot(data.frame(resid(model_drop_No_Add)), aes(sample = resid(model_drop_No_Add))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ggtitle("Q-Q Plot of one less predictor")

p5 = ggplot(data.frame(resid(model_clean)), aes(sample = resid(model_clean))) +
  stat_qq() +
  stat_qq_line(color = "red") +
  ylim(-6.25, 2.5) +
  ggtitle("Q-Q Plot of After remove outlier")

grid.arrange( p2, p3, p4, p5, ncol = 2)
```
# Final model inference and results

```{r}
#| label: tbl-final-model-regression-coef
#| tbl-cap: "Regression Coeﬀicients with 95% Confidence Intervals"
#| echo: false

# Get tidy coefficients
coef_table <- tidy(model_clean)

#Get confidence intervals
confint_df <- confint(model_clean) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("term") %>%
  rename(
    `Lower 95% CI` = `2.5 %`,
    `Upper 95% CI` = `97.5 %`
  )

# Merge coefficients with confidence intervals
full_table <- left_join(coef_table, confint_df, by = "term")

#Format p-values nicely
full_table <- full_table %>%
  mutate(
    `t value` = formatC(statistic, format = "f", digits = 2),
    `Pr(>|t|)` = format.pval(p.value, digits = 3, eps = .Machine$double.eps),
    Estimate = formatC(estimate, format = "f", digits = 4),
    `Std. Error` = formatC(std.error, format = "f", digits = 4),
    `Lower 95% CI` = formatC(`Lower 95% CI`, format = "f", digits = 4),
    `Upper 95% CI` = formatC(`Upper 95% CI`, format = "f", digits = 4)
  )

full_table <- full_table %>%
  mutate(term = case_when(
    term == "(Intercept)" ~ "(Intercept)",
    term == "School_LanguageFrench" ~ "School Language - French ",
    term == "Special_Ed_Pct" ~ "% in Special Education",
    term == "Low_Income_Transform" ~ "% in Low Income",
    term == "No_Edu_Transform" ~ "% in No Educated Parent ",
    TRUE ~ term  
  ))

full_table[, c("term", "Estimate", "Std. Error", "t value", "Pr(>|t|)", "Lower 95% CI", "Upper 95% CI")] %>%
  kable(
    col.names = c("Predictor", "Estimate", "Std. Error", "t value", "p-value", "Lower 95% CI", "Upper 95% CI"),
    caption = "Regression Coefficients with 95% Confidence Intervals",
    escape = FALSE,
    booktabs = TRUE,
    align = "lcccccc"
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "threeparttable"), 
    font_size = 9
  ) %>%
  row_spec(0, bold = TRUE)
```

## Model Interpretation 

The regression results presented in @tbl-final-model-regression-coef provide meaningful insight into how school and family level factors influence OSSLT first attempt success rates in Ontario. Notably, the language of instruction emerges as a significant predictor: schools offering instruction in French are associated with higher OSSLT performance, holding all other variables in the model constant. Specifically, French language schools are predicted to have a 0.3272 unit increase in the square root of the OSSLT first attempt pass rate compared to English language schools. The 95% confidence interval for this estimate ranges from 0.2221 to 0.4323, indicating a consistently positive effect that is statistically significant. 

In contrast, three key indicators of socioeconomic and educational disadvantage are all significantly associated with lower OSSLT performance: the percentage of students receiving special education services, the percentage of school aged children living in low income households, and the percentage of students whose parents have no degree, diploma, or certificate. 
For each one percentage point increase in students receiving special education, there is an estimated 0.0369 unit decrease in the square root of the OSSLT pass rate, with a 95% confidence interval ranging from -0.0399 to -0.0338. Likewise, each additional percentage point of students from low income households is associated with a 0.016 unit decrease, with the confidence interval ranging from -0.0213 to -0.0107. Finally, each percentage point increase in students whose parents lack post secondary education corresponds to a 0.0278 unit decrease in the transformed OSSLT outcome, with a confidence interval between -0.0349 and -0.0207. The narrow confidence intervals for all predictors suggest that the estimated effects are both precise and robust.

## Comparing with Literature

The findings from our final regression model align closely with much of the existing literature on factors influencing student literacy outcomes. Consistent with @zhang2020family, we found that both family income and parental education level are significant predictors of OSSLT success: schools with higher percentages of students from low-income households and students whose parents lack post-secondary education showed notably lower OSSLT pass rates. This supports the broader claim that socioeconomic status plays a critical role in shaping educational achievement. Similarly, our results reinforce the observations of @vaughn2014intensive, as schools with higher proportions of students receiving special education services were significantly associated with lower OSSLT outcomes, likely due to the academic challenges these students face, even with accommodations.

However, our results diverge from the expectation presented by @bernhofer2022effect, who suggest students perform better when taught in their first language. In our analysis, schools offering instruction in French had significantly higher OSSLT performance, even though the OSSLT is administered in English. This suggests that French language instruction may be associated with school environments or educational practices that contribute positively to student literacy, despite the language difference. It may also reflect broader institutional or cultural differences between French and English schools in Ontario that warrant further exploration, such as school funding models, curriculum focus, or community engagement.

Our hypothesis regarding school type was not supported in the final model. School type, which identifies whether a school is Catholic or Public, was excluded due to a lack of statistical significance as discussed above. This outcome contrasts with the findings of @cheema2024difference, who reported that private schools tend to outperform public schools in literacy achievement. While we expected Catholic schools to demonstrate higher OSSLT performance due to structured curricula or the potential influence of religious education resources, our model did not find a meaningful difference once other variables were accounted for. This suggests that variation in OSSLT performance across schools is more strongly explained by socioeconomic and instructional factors than by school type alone. 


```{r}
#| label: tbl-final-model-fit
#| tbl-cap: "Model Fit Statistics"
#| echo: false

model_metrics <- data.frame(
  Metric = c(
    "R-squared",
    "Adjusted R-squared",
    "AIC",
    "BIC",
    "AICc",
    "Residual Std. Error"
  ),
  Value = c(
    summary(model_clean)$r.squared,
    summary(model_clean)$adj.r.squared,
    extractAIC(model_clean)[2],
    BIC(model_clean),
    AICc(model_clean),
    summary(model_clean)$sigma
  )
)

# Display table
kable(model_metrics, digits = 4)
```

## Model Performance Assessment
The performance of the final multiple linear regression model, as shown in @tbl-final-model-fit, can be evaluated using several statistical metrics that assess both goodness of fit and model parsimony. One of the most interpretable metrics is the R-squared value, which in this model is 0.5429. This indicates that approximately 54.3% of the variation in the square root of the OSSLT first-attempt pass rate is explained by the predictors included in the model. In the context of educational research, where student performance can be influenced by many unmeasured social, psychological, and institutional factors, an R-squared value above 0.5 is considered relatively strong. It suggests that the model captures a substantial portion of the meaningful variance across schools in Ontario.

The Adjusted R-squared value, which accounts for the number of predictors in the model, is 0.5404. While slightly lower than the unadjusted R-squared, this is expected and confirms that the included predictors contribute meaningfully to explaining the outcome without overfitting the data. The minimal difference between the two values suggests that the model achieves a good balance between explanatory power and complexity. This strengthens confidence that the model's performance is not artificially inflated by the number of predictors used.

Beyond explanatory power, model selection criteria such as the Akaike Information Criterion (AIC), the corrected AIC (AICc), and the Bayesian Information Criterion (BIC) provide important insights into model efficiency and generalizability. Both AIC and AICc are measures of model fit that penalize complexity, with lower values indicating better-fitting models. In our model, the AIC is -1174.5494 and the AICc is 902.0548. 

The distinction between AIC and AICc lies in their intended use: AICc is a bias-corrected version of AIC that is particularly useful when the sample size is small or when the number of estimated parameters is a moderate to large fraction of the sample size. According to the rule of thumb provided by @burnham2004multimodel, AICc is preferred over AIC when the sample size $n \leq 40(p + 2)$ where p is the number of predictors. Based on this criterion, our dataset includes around eight hundred observations and only four predictors, which indicates that AIC is a more suitable measure for evaluating our model.


\newpage

\appendix

# Appendix
```{r}
library(gridExtra)

# Add fitted values to df_clean
df_clean$fitted_values <- model$fitted.values

# Function for ggplot with regression line
ggplot_with_abline <- function(df, xcol, xlabel) {
  ggplot(df, aes(x = .data[[xcol]], y = fitted_values)) +
    geom_point(color = "blue") +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    labs(title = paste(xlabel, "vs y-hat"), x = xlabel, y = "y-hat") +
    theme_minimal()
}

# Create plots
 # Create boxplot for School_Type
p1 <- ggplot(df_clean, aes(x = School_Type, y = fitted_values)) +
  geom_boxplot(aes(fill = School_Type), outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
  labs(title = "School Type vs y-hat", x = "School Type", y = "y-hat") +
  theme_minimal()

# Create boxplot for School_Language
p2 <- ggplot(df_clean, aes(x = School_Language, y = fitted_values)) +
  geom_boxplot(aes(fill = School_Language), outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
  labs(title = "School Language vs y-hat", x = "School Language", y = "y-hat") +
  theme_minimal()

p3 <- ggplot_with_abline(df_clean, "Special_Ed_Pct", "Special Ed Pct")
p4 <- ggplot_with_abline(df_clean, "Low_Income_Pct", "Low Income Pct")


p5 <- ggplot_with_abline(df_clean, "No_Parent_Degree_Pct", "No Parent Degree Pct")

# Arrange plots in a grid
grid.arrange(p1, p2, p3, p4, p5, ncol = 3, nrow = 2)
```
\newpage
# References
